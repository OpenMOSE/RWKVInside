# RWKVInside

# HybridModel Overview

```mermaid
flowchart TB
    subgraph HybridModel
        input[Input] --> Embeddings
        Embeddings --> L0[Layer 0]
        
        subgraph Layer0[Layer 0]
            direction LR
            A0[AttentionWrapper] --> MLP0[MLP]
            subgraph AW0[AttentionWrapper 0]
                direction TB
                TA0[TeacherAttn\nFrozen] --> |Compare|SA0[StudentAttn\nRWKV TimeMixer]
                SA0 --> |Generate|VF0[v_first state 0]
            end
        end
        
        subgraph Layer1[Layer 1]
            direction LR
            A1[AttentionWrapper] --> MLP1[MLP]
            subgraph AW1[AttentionWrapper 1]
                direction TB
                TA1[TeacherAttn\nFrozen] --> |Compare|SA1[StudentAttn\nRWKV TimeMixer]
                SA1 --> |Generate|VF1[v_first state 1]
            end
        end
        
        subgraph LayerN[Layer N]
            direction LR
            AN[AttentionWrapper] --> MLPN[MLP]
            subgraph AWN[AttentionWrapper N]
                direction TB
                TAN[TeacherAttn\nFrozen] --> |Compare|SAN[StudentAttn\nRWKV TimeMixer]
                SAN --> |Generate|VFN[v_first state N]
            end
        end
        
        L0 --> L1[Layer 1]
        L1 --> LN[...]
        LN --> LayerN
        LayerN --> output[Output]
        
        subgraph VFirstHolder[VFirstHolder Distributed State]
            direction TB
            VS[Shared State\nShape: world_size x batch_size x seq_length x hidden_size]
        end
        
        %% v_first flow connections
        VF0 --> |Update Rank Slice|VS
        VS --> |Next Layer Input|VF1
        VF1 --> |Update Rank Slice|VS
        VS --> |Next Layer Input|VFN
        VFN --> |Update Rank Slice|VS
    end
    
    classDef default fill:#f9f9f9,stroke:#333,stroke-width:2px;
    classDef attention fill:#e1f5fe,stroke:#01579b,stroke-width:2px;
    classDef vfirst fill:#f3e5f5,stroke:#4a148c,stroke-width:2px;
    classDef mlp fill:#f1f8e9,stroke:#33691e,stroke-width:2px;
    
    class A0,A1,AN attention;
    class VF0,VF1,VFN,VS vfirst;
    class MLP0,MLP1,MLPN mlp;
```

## Training Process

```mermaid
graph TD
    A[ÂºÄÂßã] --> B[ÂèÇÊï∞ÂàùÂßãÂåñ]
    B --> C[Âä†ËΩΩÈÖçÁΩÆÊñá‰ª∂]
    C --> D[ÂàùÂßãÂåñÊ®°ÂûãÂíåÂàÜËØçÂô®]
    D --> E[ËÆæÁΩÆDeepSpeedÈÖçÁΩÆ]
    
    E --> F{Âà§Êñ≠ËÆ≠ÁªÉÈò∂ÊÆµ<br>stage 1/2/3}
    
    F -->|Stage 1| G1[ÂàùÂßãÂåñÊïôÂ∏àÊ≥®ÊÑèÂäõÂàóË°®]
    F -->|Stage 2| G2[ÂàùÂßãÂåñÂÆåÊï¥ÊïôÂ∏àÊ®°Âûã]
    F -->|Stage 3/SFT| G3[Ë∑≥ËøáÊïôÂ∏àÊ®°ÂûãÂàùÂßãÂåñ]
    
    G1 --> H[ÂáÜÂ§áÊï∞ÊçÆÂä†ËΩΩÂô®]
    G2 --> H
    G3 --> H
    
    H --> I[ÂºÄÂßãËÆ≠ÁªÉÂæ™ÁéØ]
    
    subgraph ËÆ≠ÁªÉÂæ™ÁéØ
        I --> J[Êõ¥Êñ∞Â≠¶‰π†ÁéáÂíåÊùÉÈáçË°∞Âáè]
        J --> K[ÂâçÂêë‰º†Êí≠]
        K --> L[ËÆ°ÁÆóÊçüÂ§±]
        L --> M[ÂèçÂêë‰º†Êí≠]
        M --> N{ÊòØÂê¶Á¥ØÁßØÊ≠•È™§}
        N -->|ÊòØ| O[‰ºòÂåñÂô®Ê≠•Ëøõ]
        N -->|Âê¶| P[ÁªßÁª≠‰∏ã‰∏ÄÊâπÊ¨°]
        O --> Q{Ê£ÄÊü•‰øùÂ≠òÊù°‰ª∂}
        P --> J
        Q -->|Êª°Ë∂≥| R[‰øùÂ≠òÊ£ÄÊü•ÁÇπ]
        Q -->|‰∏çÊª°Ë∂≥| S{Ê£ÄÊü•ÁªàÊ≠¢Êù°‰ª∂}
        R --> S
        S -->|ÁªßÁª≠| J
        S -->|ÁªàÊ≠¢| T[ÁªìÊùüËÆ≠ÁªÉ]
    end
    
    subgraph ÊçüÂ§±ËÆ°ÁÆó
        L --> L1[ÊïôÂ∏àÊçüÂ§±]
        L --> L2[KLÊï£Â∫¶ÊçüÂ§±]
        L --> L3[Â≠¶Áîü‰∫§ÂèâÁÜµÊçüÂ§±]
    end
```
### Training shell

The training shell is the train.sh. Please  refer the train_memo.txt for more details for training in different stages.

## RL Training Process

```mermaid
flowchart TB
    subgraph Init["ÂàùÂßãÂåñÈò∂ÊÆµ"]
        A[Ëß£ÊûêÂëΩ‰ª§Ë°åÂèÇÊï∞] --> B[ËÆæÁΩÆÂàÜÂ∏ÉÂºèÁéØÂ¢É]
        B --> C[Âä†ËΩΩÂàÜËØçÂô®]
        C --> D[Âä†ËΩΩÊï∞ÊçÆÈõÜ]
        D --> E[ÂàõÂª∫DataLoader]
        E --> F[ÂàùÂßãÂåñDeepSpeedÈÖçÁΩÆ]
    end

    subgraph Models["Ê®°ÂûãÂàùÂßãÂåñ"]
        F --> G[ÂàùÂßãÂåñ‰∏ªÊ®°Âûã]
        G --> H[ÂàùÂßãÂåñÂèÇËÄÉÊ®°Âûã]
        H --> J[ÈÖçÁΩÆ‰ºòÂåñÂô®]
    end

    subgraph Training["ËÆ≠ÁªÉÂæ™ÁéØ"]
        J --> K[ËøõÂÖ•ËÆ≠ÁªÉËΩÆÊ¨°]
        K --> L[ÊâπÊ¨°ËÆ≠ÁªÉ]
        L --> M{ÊòØÂê¶ÈúÄË¶Å‰øùÂ≠òÊ£ÄÊü•ÁÇπ}
        M -->|ÊòØ| N[‰øùÂ≠òÊ®°ÂûãÊ£ÄÊü•ÁÇπ]
        M -->|Âê¶| O[ÁªßÁª≠ËÆ≠ÁªÉ]
        N --> O
        O --> P{ËΩÆÊ¨°ÁªìÊùü?}
        P -->|Âê¶| L
        P -->|ÊòØ| Q[‰øùÂ≠òËΩÆÊ¨°Ê£ÄÊü•ÁÇπ]
        Q --> R{ÂÖ®ÈÉ®ËΩÆÊ¨°ÂÆåÊàê?}
        R -->|Âê¶| K
        R -->|ÊòØ| S[ËÆ≠ÁªÉÁªìÊùü]
    end

    subgraph TrainStep["ÊâπÊ¨°ËÆ≠ÁªÉÊ≠•È™§(GRPOTrainer)"]
        L --> T[ÁîüÊàêÂÆåÊàêÂÜÖÂÆπ]
        T --> U[ËÆ°ÁÆóÂ•ñÂä±]
        U --> V[ËÆ°ÁÆóKLÊï£Â∫¶]
        V --> W[Â§öÊ¨°Êõ¥Êñ∞Ëø≠‰ª£]
        W --> X[ËøîÂõûËÆ≠ÁªÉÊåáÊ†á]
    end

    subgraph Metrics["ÊåáÊ†áËÆ∞ÂΩï"]
        X --> Y[Êõ¥Êñ∞Á¥ØËÆ°ÁªüËÆ°]
        Y --> Z[ËÆ∞ÂΩïÂà∞WandB]
    end
```

### GRPO Algorithm from https://arxiv.org/pdf/2402.03300v3

![alt text](image-1.png)

The implementation of GRPO algorithm is rl/grpo_trainer.py , please refer the code for more details. TRL just eliminates the GRPO iteration and ignore the memory efficiency for large data in relative small machines. That's why we implement our own GRPO algorithm.
# Training üî•

## Data preparation ü§ó
- prepare input Raw data in [input_Raw_data_dir]
### Datasets Format(Jsonl format)
{"text": "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process are enclosed within <think> </think>, i.e., <think> reasoning process here </think>**Final Answer:**\nanswer here. 
<ÔΩúUserÔΩú>......<ÔΩúAssistantÔΩú>\n<think>\n......\n</think>\n\n......<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>"}

## Build Environment ü§Ø
```bash
sudo apt install python3.12-venv
```
```bash
python -m venv .venv --system-site-packages
source .venv/bin/activate
pip install torch torchvision torchaudio gradio rwkv-fla accelerate deepspeed wandb 
git clone https://github.com/uniartisan/transformers.git
cd ./transformers
pip install . 
```

## Train model üòã
- AMD hipcc Compile extra_cuda_cflags: '-xhip', '-fopenmp', '-ffast-math', '-O3', '--offload-arch=gfx1100','-munsafe-fp-atomics'
- Nvidia nvcc Compile extra_cuda_cflags: '-res-usage', '--maxrregcount 60', '--use_fast_math', '-O3', '-Xptxas -O3'

### Stage 1 
- Training for Qwen 0.5B with Norm

```bash
sh train.sh -c configs/qwen_0.5b.yaml -l 0.0001 -f 0.00001 -m 2048 -b 2 -r "[input_Raw_data_dir_1] [input_Raw_data_dir_2] [input_Raw_data_dir_3]..." -o [output_model_path]  -g 1 -F 0 -d 1 -t 1000_000_000 -T 0.2 -R v7 -s 1 -M 1 -G [Use GPU number]
```
### Stage 2
- Training for Qwen 0.5B with Norm
```bash
sh train.sh -c configs/qwen_0.5b.yaml -l 0.0001 -f 0.00001 -m 2048 -b 2 -r "[input_Raw_data_dir_1] [input_Raw_data_dir_2] [input_Raw_data_dir_3]..." -o [Stage_1_output_model_dir]  -g 1 -F 0 -d 1 -t 1000_000_000 -T 0.2 -R v7 -s 2 -k [output_deepspeed_model_weight_dir] -M 1 -G [Use GPU number]
```

- Training for Qwen 0.5B with Norm and freez mlp
```bash
sh train.sh -c configs/qwen_0.5b.yaml -l 0.0001 -f 0.00001 -m 2048 -b 2 -r "[input_Raw_data_dir_1] [input_Raw_data_dir_2] [input_Raw_data_dir_3]..." -o [Stage_1_output_model_dir]  -g 1 -F 0 -d 1 -t 1000_000_000 -T 0.2 -R v7 -s 2 -k [output_deepspeed_model_weight_dir] -M 1 -z 1 -G [Use GPU number]
```

- Training for Qwen 0.5B with Norm and freez mlp and use another teacher model
```bash
sh train.sh -c configs/qwen_0.5b.yaml -l 0.0001 -f 0.00001 -m 2048 -b 2 -r "[input_Raw_data_dir_1] [input_Raw_data_dir_2] [input_Raw_data_dir_3]..." -o [Stage_1_output_model_dir]  -g 1 -F 0 -d 1 -t 1000_000_000 -T 0.2 -R v7 -s 2 -k [output_deepspeed_model_weight_dir] -M 1 -z 1 -i [Another_Instruct_teacher_model_dir_with_config] -G [Use GPU number]
```

[Jump to document describes the usage of the `train.sh` script for model training with DeepSpeed](./Train.md)

### Save checkpoint 

```bash
python ./train_scripts/save_checkpoint.py [checkpoint_dir] [output_dir]
```

### If you want to train on AMD GPU (Test passed on Ubuntu 24.04 with W7900)ü§Ø
#### 1. [install ROCM Doc on Official Documentation](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/install-methods/package-manager-index.html)
#### 2. Build Environment
```bash
sudo apt install python3.12-venv
```
```bash
python -m venv .venv --system-site-packages
source .venv/bin/activate
```
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2.4
pip install gradio rwkv-fla accelerate deepspeed 
```
```bash
git clone https://github.com/uniartisan/transformers.git
cd ./transformers
pip install . 
```
#### 3. Because cupy only supports up to ROCm5.0 so we need to remove ‚Äúimpot cupy‚Äù
#### 4. Modify ./cuda/wkv7_cuda.cu 
```cpp
using bf = __nv_bfloat16;
__device__ inline float to_float(const bf & u) { return __bfloat162float(u); }
__device__ inline bf to_bf(const float & u) { return __float2bfloat16_rn(u); }
```
to
```cpp
using bf = __nv_bfloat16;
__device__ inline float to_float(const bf & u) { return __bfloat162float(u); }
__device__ inline bf to_bf(const float & u) { return __float2bfloat16(u); }
```
#### 5. Change All deepspeed ```backend='nccl'``` to ```backend='rccl'``` in ./train_scripts/train_hybrid_deepspeed.py 
#### 6.Change All 
```python
extra_cuda_cflags=[f'-D_C_={HEAD_SIZE}',"-res-usage", "--use_fast_math", "-O3", "-Xptxas -O3", "--extra-device-vectorization"]
```
to
```python
[f'-D_C_={HEAD_SIZE}', f"-D_CHUNK_LEN_={CHUNK_LEN}", '-xhip', '-fopenmp', '-ffast-math', '-O3', '--offload-arch=gfx1100','-munsafe-fp-atomics']
```
in ```./rwkv_inside/TimeMixer.py```

# Infrence on Nvidia GPU üöÄ

## prepare model
1. convert deepspeed model to pytorch format  

```bash
python ./train_scripts/convert_pt.py --model_path [input_model_dir_path] --output_path [output_model_dir_path] --original_model_path [dir_of_the_original_qwen_model]
```
2. convert pytorch model to hf format for Infrence

```bash
python ./test/convert_2_hf.py --config_file [input_config_file] --ckpt_file [input_deepspeed_model_weight_dir] --output_config_dir [output_config_dir]
```

## Test model
1. test chat in cli
```bash
python ./test/test_chat_cli.py [model_config_dir]
```
2. test test caht in gradio with thinking
```bash
python ./test/test_hf_gradio_thinking.py [model_config_dir]
```
3. test chat in gradio with fp16
```bash
python ./test/test_hf_gradio.py [model_config_dir]
```
4. test by a single prompt with fp16
```bash
python ./test/test_hf.py [model_config_dir]
```

# Infrence on AMD Radeon GPU (Test passed on Ubuntu 24.04 with W7900 & RX6750xt)üöÄ
1. [install ROCM Doc on Official Documentation](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/install-methods/package-manager-index.html)
2. Build Environment
```bash
sudo apt install python3.12-venv
```
```bash
python -m venv .venv --system-site-packages
source .venv/bin/activate
```
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2.4
pip install gradio rwkv-fla accelerate deepspeed cupy
```
```bash
git clone https://github.com/uniartisan/transformers.git
cd ./transformers
pip install . 
```
## prepare model
1. convert deepspeed model to hf format 
```bash
python test/convert_2_hf.py --config_file [input_config_file] --ckpt_file [input_deepspeed_model_weight_dir] --output_config_dir [output_config_dir]
```

## Test model
1. test chat in cli
```bash
python ./test/test_chat_cli.py [model_config_dir]
```
2. test test caht in gradio with thinking
```bash
python ./test/test_hf_gradio_thinking.py [model_config_dir]
```
3. test chat in gradio with fp16
```bash
python ./test/test_hf_gradio.py [model_config_dir]
```
4. test by a single prompt with fp16
```bash
python ./test/test_hf.py [model_config_dir]
```

# Infrence on AMD Radeon GPU By Llama.cpp

```bash
git clone https://github.com/MollySophia/llama.cpp.git -b rwkv-v7
cd llama.cpp
HIPCXX="$(hipconfig -l)/clang" HIP_PATH="$(hipconfig -R)" \
    cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx1030 -DCMAKE_BUILD_TYPE=Release \
    && cmake --build build --config Release -- -j 16
cd ./build/bin 
```
### transform safetensor model to gguf 
```bash
python ./convert_hf_to_gguf.py [model_dir]
```
### model Quantization
```bash
./llama-quantize [model_dir] [Quantization accuracy]
```
### Infrence model in Webui By llama-server
```bash
/llama-server -m [model_dir] -t [use_cpu_thread_number] -ngl 99 --host [host_number] --port [port_number]
```
```Radeon 7000 series use gfx1100 & Radeon 6000 series use gfx1030```
# Infrence on Nvidia GPU By Llama.cpp
```bash
git clone https://github.com/MollySophia/llama.cpp.git -b rwkv-v7
cd llama.cpp
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
cd ./build/bin 
```
### transform safetensor model to gguf 
```bash
python ./convert_hf_to_gguf.py [model_dir]
```
### model Quantization
```bash
./llama-quantize [model_dir] [Quantization_accuracy]
```
### Infrence model in Webui By llama-server
```bash
/llama-server -m [model_dir] -t [use_cpu_thread_number] -ngl 99 --host [host_number] --port [port_number]
```