deepspeed==0.14.5
wandb
prompt_toolkit
pandas
triton==3.1.0
accelerate
transformers
einops
h5py
git+https://github.com/fla-org/flash-linear-attention