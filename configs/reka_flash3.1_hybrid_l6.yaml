RWKV:
  layers: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43]
  grad_cp: 1
  #transformer_layers: [-1]
  transformer_layers: [8,15,22,29,36,43]
disable_qk_norm: True
freeze_attention: 0
hybrid_attention_layers: 0
freeze_hybrid_attention: 0
allow_quant_frozen_layers: 1
quant_mode: none
peftmode: full
peft_r: 32
peft_scaling: 0.5
peft_dropout: 0.01
mlp_quant_mode: int8
bnb_optimizer_mode: 0

Llama:
  model_id: /home/llm/reka-flash-3.1
is_llama_ffn: True
kl_weight: 1
ce_weight: 0
model_file: reka-flash-3.1
is_rwkv_att_only: True
teach_mode:
  is_client: False
is_all_labels_kl: True