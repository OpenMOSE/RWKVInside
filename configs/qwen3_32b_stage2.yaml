RWKV:
  layers: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63]
  grad_cp: 1
 # transformer_layers: [-1]
  transformer_layers: [5,11,17,23,28,33,38,43,48,53,58,63]
architecture: hxa079
disable_qk_norm: False
freeze_attention: 0
hybrid_attention_layers: 0
freeze_hybrid_attention: 0
allow_quant_frozen_layers: 1
quant_mode: none
peftmode: full
peft_r: 32
peft_scaling: 0.5
peft_dropout: 0.01
mlp_quant_mode: int8
bnb_optimizer_mode: 0

Llama:
  model_id: /workspace/llm/Qwen3-32B
is_llama_ffn: True
kl_weight: 1
ce_weight: 0
model_file: Qwen3-32B
is_rwkv_att_only: True
teach_mode:
  is_client: False
is_all_labels_kl: True