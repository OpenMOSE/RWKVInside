RWKV:
  layers: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47]
  grad_cp: 1
  #transformer_layers: [-1]
  transformer_layers: [6, 14, 18, 26, 30, 38, 45, 47]
architecture: hxa07b
disable_qk_norm: False
freeze_attention: 0
hybrid_attention_layers: 0
freeze_hybrid_attention: 0
allow_quant_frozen_layers: 1
quant_mode: none
peftmode: full
peft_r: 32
peft_scaling: 0.5
peft_dropout: 0.01
mlp_quant_mode: int8
bnb_optimizer_mode: 0

Llama:
  model_id: /workspace/llm/Qwen3-30B-A3B-Instruct-2507/
is_llama_ffn: True
kl_weight: 1
ce_weight: 0
model_file: Qwen3-30B-A3B-Instruct-2507
is_rwkv_att_only: True
teach_mode:
  is_client: False
is_all_labels_kl: True