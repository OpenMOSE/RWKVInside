RWKV:
  layers: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27]
  grad_cp: 1
  #transformer_layers: [-1]
  transformer_layers: [7,12,17,22,27]
architecture: hxa079
disable_qk_norm: False
freeze_attention: 0
hybrid_attention_layers: 0
freeze_hybrid_attention: 1
allow_quant_frozen_layers: 1
quant_mode: none
peftmode: full
peft_r: 32
peft_scaling: 0.5
peft_dropout: 0.01
mlp_quant_mode: int8
bnb_optimizer_mode: 0

Llama:
  model_id: /workspace/llm/Qwen3-0.6B
is_llama_ffn: True
kl_weight: 1
ce_weight: 0
model_file: Qwen3-14B
is_rwkv_att_only: True
teach_mode:
  is_client: False
is_all_labels_kl: True