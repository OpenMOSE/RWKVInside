RWKV:
  layers: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35]
  grad_cp: 1
  transformer_layers: [7,15,23,31,33,35]

freeze_attention: 0
hybrid_attention_layers: 0
freeze_hybrid_attention: 0
allow_quant_frozen_layers: 1
quant_mode: int8
peftmode: full
peft_r: 32
peft_scaling: 0.5
peft_dropout: 0.01
mlp_quant_mode: nf4
bnb_optimizer_mode: 1

Llama:
  model_id: /home/client/Projects/llm/Qwen3-8B/
is_llama_ffn: True
kl_weight: 1
ce_weight: 0
model_file: QWen3-8B
is_rwkv_att_only: True
teach_mode:
  is_client: False
is_all_labels_kl: True